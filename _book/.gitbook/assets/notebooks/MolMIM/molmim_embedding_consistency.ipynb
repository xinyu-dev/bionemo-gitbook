{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b43cec",
   "metadata": {},
   "source": [
    "# MolMIM Embedding Consistency\n",
    "\n",
    "Make sure you have weights in `/workspace/bionemo/models`. This notebooks uses the interactive inference method without the need to launch local triton server explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ca44898eb4ca64",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f92202a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model configuration at: /workspace/bionemo/examples/molecule/molmim/conf\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "try:\n",
    "    BIONEMO_HOME: Path = Path(os.environ['BIONEMO_HOME']).absolute()\n",
    "except KeyError:\n",
    "    print(\"Must have BIONEMO_HOME set in the environment! See docs for instructions.\")\n",
    "    raise\n",
    "\n",
    "config_path = BIONEMO_HOME / \"examples\" / \"molecule\" / \"molmim\" / \"conf\"\n",
    "print(f\"Using model configuration at: {config_path}\")\n",
    "assert config_path.is_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38725cbe",
   "metadata": {},
   "source": [
    "### Setup and Test Data\n",
    "\n",
    "`InferenceWrapper` is an adaptor that allows interaction with inference service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16f30d8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-23 13:57:02 utils:490] pytorch DDP is not initialized. Initializing with pytorch-lightening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-23 13:57:02 utils:333] Restoring model from /workspace/bionemo/models/molecule/molmim/molmim_70m_24_3.nemo\n",
      "[NeMo I 2024-05-23 13:57:02 utils:337] Loading model class: bionemo.model.molecule.molmim.molmim_model.MolMIMModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive mode selected, using strategy='auto'\n",
      "[NeMo I 2024-05-23 13:57:02 exp_manager:394] Experiments will be logged at /workspace/bionemo/examples/molecule/molmim/nemo_experiments/MolMIM_Inference/2024-05-23_13-57-02\n",
      "[NeMo I 2024-05-23 13:57:02 exp_manager:835] TensorboardLogger has been set up\n",
      "[NeMo I 2024-05-23 13:57:02 utils:306] \n",
      "    \n",
      "    ************** Trainer configuration ***********\n",
      "[NeMo I 2024-05-23 13:57:02 utils:307] \n",
      "    name: MolMIM_Inference\n",
      "    desc: Minimum configuration for initializing a MolMIM model for inference.\n",
      "    trainer:\n",
      "      precision: 16-mixed\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      logger: false\n",
      "      accumulate_grad_batches: 1\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_checkpoint_callback: false\n",
      "    model:\n",
      "      encoder:\n",
      "        num_layers: 6\n",
      "        hidden_size: 512\n",
      "        ffn_hidden_size: 2048\n",
      "        num_attention_heads: 8\n",
      "        init_method_std: 0.02\n",
      "        hidden_dropout: 0.1\n",
      "        attention_dropout: 0.1\n",
      "        ffn_dropout: 0.0\n",
      "        position_embedding_type: learned_absolute\n",
      "        relative_attention_num_buckets: 32\n",
      "        relative_attention_max_distance: 128\n",
      "        relative_position_bias_self_attention_only: true\n",
      "        kv_channels: null\n",
      "        apply_query_key_layer_scaling: false\n",
      "        layernorm_epsilon: 1.0e-05\n",
      "        persist_layer_norm: true\n",
      "        bias_activation_fusion: true\n",
      "        grad_div_ar_fusion: true\n",
      "        masked_softmax_fusion: true\n",
      "        bias_dropout_add_fusion: true\n",
      "        bias: true\n",
      "        normalization: layernorm\n",
      "        arch: perceiver\n",
      "        activation: gelu\n",
      "        headscale: false\n",
      "        transformer_block_type: pre_ln\n",
      "        hidden_steps: 1\n",
      "        num_self_attention_per_cross_attention: 1\n",
      "        openai_gelu: false\n",
      "        onnx_safe: false\n",
      "        fp32_residual_connection: false\n",
      "        activations_checkpoint_method: null\n",
      "        activations_checkpoint_num_layers: 1\n",
      "        activations_checkpoint_granularity: null\n",
      "        megatron_legacy: false\n",
      "        normalize_attention_scores: true\n",
      "        num_moe_experts: 1\n",
      "        moe_frequency: 1\n",
      "        moe_dropout: 0.0\n",
      "        use_flash_attention: false\n",
      "      decoder:\n",
      "        num_layers: 6\n",
      "        hidden_size: 512\n",
      "        ffn_hidden_size: 2048\n",
      "        num_attention_heads: 8\n",
      "        init_method_std: 0.02\n",
      "        hidden_dropout: 0.1\n",
      "        attention_dropout: 0.1\n",
      "        ffn_dropout: 0.0\n",
      "        position_embedding_type: learned_absolute\n",
      "        relative_attention_num_buckets: 32\n",
      "        relative_attention_max_distance: 128\n",
      "        relative_position_bias_self_attention_only: true\n",
      "        kv_channels: null\n",
      "        apply_query_key_layer_scaling: false\n",
      "        layernorm_epsilon: 1.0e-05\n",
      "        persist_layer_norm: true\n",
      "        bias_activation_fusion: true\n",
      "        grad_div_ar_fusion: true\n",
      "        masked_softmax_fusion: true\n",
      "        bias_dropout_add_fusion: true\n",
      "        bias: true\n",
      "        normalization: layernorm\n",
      "        arch: transformer\n",
      "        activation: gelu\n",
      "        headscale: false\n",
      "        transformer_block_type: pre_ln\n",
      "        hidden_steps: 32\n",
      "        num_self_attention_per_cross_attention: 1\n",
      "        openai_gelu: false\n",
      "        onnx_safe: false\n",
      "        fp32_residual_connection: false\n",
      "        activations_checkpoint_method: null\n",
      "        activations_checkpoint_num_layers: 1\n",
      "        activations_checkpoint_granularity: null\n",
      "        megatron_legacy: false\n",
      "        normalize_attention_scores: true\n",
      "        num_moe_experts: 1\n",
      "        moe_frequency: 1\n",
      "        moe_dropout: 0.0\n",
      "        use_flash_attention: false\n",
      "      name: MolMIM-small\n",
      "      micro_batch_size: ${model.data.batch_size}\n",
      "      global_batch_size: 128\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      resume_from_checkpoint: null\n",
      "      pipeline_model_parallel_split_rank: 0\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      megatron_amp_O2: false\n",
      "      seq_length: 128\n",
      "      max_position_embeddings: 128\n",
      "      gradient_as_bucket_view: true\n",
      "      bias_gelu_fusion: true\n",
      "      share_token_embeddings: true\n",
      "      share_decoder_tokens_head_embeddings: false\n",
      "      hidden_size: 512\n",
      "      training_callbacks: []\n",
      "      hiddens:\n",
      "        enc_output_name: z\n",
      "        enc_inference_output_name: z_mean\n",
      "        token_aggregation_method: mean\n",
      "        hidden_aggregation_method: mean\n",
      "        transform:\n",
      "          q_z_given_x:\n",
      "            cls_name: sampled_var_cond_gaussian\n",
      "            hidden_size: 512\n",
      "            min_logvar: -6.0\n",
      "            max_logvar: 0.0\n",
      "            map_var_to_hiddens: false\n",
      "        loss:\n",
      "          mim:\n",
      "            cls_name: a_mim\n",
      "            loss_weight: 1.0\n",
      "      tokenizer:\n",
      "        library: regex\n",
      "        type: null\n",
      "        model: nemo:048c1f797f464dd5b6a90f60f9405827_molmim.model\n",
      "        vocab_file: nemo:dd344353154640acbbaea1d4536fa7d0_molmim.vocab\n",
      "        merge_file: null\n",
      "        vocab_path: ${oc.env:BIONEMO_HOME}/tokenizers/molecule/molmim/vocab/molmim.vocab\n",
      "        model_path: ${oc.env:BIONEMO_HOME}/tokenizers/molecule/molmim/vocab/molmim.model\n",
      "      data:\n",
      "        links_file: /workspace/bionemo/examples/molecule/megamolbart/dataset/ZINC-downloader.txt\n",
      "        dataset_path: ${oc.env:BIONEMO_HOME}/examples/tests/test_data/molecule/physchem/SAMPL/test/x000\n",
      "        dataset:\n",
      "          train: x_OP_000..175_CL_\n",
      "          test: x_OP_000..175_CL_\n",
      "          val: x_OP_000..004_CL_\n",
      "        canonicalize_target_smile: true\n",
      "        canonicalize_encoder_input: true\n",
      "        canonicalize_decoder_output: true\n",
      "        encoder_augment: false\n",
      "        decoder_independent_augment: false\n",
      "        encoder_mask: false\n",
      "        decoder_mask: false\n",
      "        mask_prob: 0.0\n",
      "        span_lambda: 3.0\n",
      "        micro_batch_size: 2048\n",
      "        num_workers: 4\n",
      "        dataloader_type: single\n",
      "        max_seq_length: 128\n",
      "        seed: 42\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        data_impl: ''\n",
      "        index_mapping_type: online\n",
      "        data_impl_kwargs:\n",
      "          csv_mmap:\n",
      "            newline_int: 10\n",
      "            header_lines: 1\n",
      "            workers: 10\n",
      "            sort_dataset_paths: true\n",
      "            data_sep: ','\n",
      "            data_col: 1\n",
      "          csv_fields_mmap:\n",
      "            newline_int: 10\n",
      "            header_lines: 1\n",
      "            workers: null\n",
      "            sort_dataset_paths: false\n",
      "            data_sep: ','\n",
      "            data_fields:\n",
      "              id: 0\n",
      "              sequence: 1\n",
      "          fasta_fields_mmap:\n",
      "            data_fields:\n",
      "              id: 0\n",
      "              sequence: 1\n",
      "        use_upsampling: true\n",
      "        index_mapping_dir: null\n",
      "        batch_size: 128\n",
      "        output_fname: ''\n",
      "        data_fields_map:\n",
      "          sequence: smiles\n",
      "          id: iupac\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0005\n",
      "        weight_decay: 0.001\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 10000.0\n",
      "          constant_steps: 50000.0\n",
      "          max_steps: 1000000\n",
      "          min_lr: 5.0e-05\n",
      "      dwnstr_task_validation:\n",
      "        enabled: false\n",
      "        dataset:\n",
      "          class: bionemo.model.core.dwnstr_task_callbacks.SingleValuePredictionCallback\n",
      "          task_type: regression\n",
      "          infer_target: bionemo.model.molecule.molmim.infer.MolMIMInference\n",
      "          max_seq_length: 128\n",
      "          emb_batch_size: 128\n",
      "          batch_size: 128\n",
      "          num_epochs: 10\n",
      "          shuffle: true\n",
      "          num_workers: 8\n",
      "          dataset_path: /data/physchem/\n",
      "          task_name: SAMPL\n",
      "          dataset:\n",
      "            train: x000\n",
      "            test: x000\n",
      "          sequence_column: smiles\n",
      "          target_column: expt\n",
      "          random_seed: 1234\n",
      "          optim:\n",
      "            name: adam\n",
      "            lr: 0.0001\n",
      "            betas:\n",
      "            - 0.9\n",
      "            - 0.999\n",
      "            eps: 1.0e-08\n",
      "            weight_decay: 0.01\n",
      "            sched:\n",
      "              name: WarmupAnnealing\n",
      "              min_lr: 1.0e-05\n",
      "              last_epoch: -1\n",
      "              warmup_ratio: 0.01\n",
      "              max_steps: 1000\n",
      "      precision: 32\n",
      "      target: bionemo.model.molecule.molmim.molmim_model.MolMIMModel\n",
      "      nemo_version: 1.22.0\n",
      "      downstream_task:\n",
      "        restore_from_path: ${oc.env:BIONEMO_HOME}/models/molecule/molmim/molmim_70m_24_3.nemo\n",
      "        outputs:\n",
      "        - embeddings\n",
      "    target: bionemo.model.molecule.molmim.molmim_model.MolMIMModel\n",
      "    infer_target: bionemo.model.molecule.molmim.infer.MolMIMInference\n",
      "    formatters:\n",
      "      simple:\n",
      "        format: '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'\n",
      "    handlers:\n",
      "      console:\n",
      "        class: logging.StreamHandler\n",
      "        formatter: simple\n",
      "        stream: ext://sys.stdout\n",
      "      file:\n",
      "        class: logging.FileHandler\n",
      "        formatter: simple\n",
      "        filename: /logs/inference.log\n",
      "    root:\n",
      "      level: INFO\n",
      "      handlers:\n",
      "      - console\n",
      "    disable_existing_loggers: false\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-23 13:57:03 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_init:297] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 megatron_base_model:821] The model: MolMIMModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-23 13:57:03 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-23 13:57:03 tokenizer_utils:199] Using regex tokenization\n",
      "[NeMo I 2024-05-23 13:57:03 regex_tokenizer:240] Loading vocabulary from file = /tmp/tmpchettrbs/dd344353154640acbbaea1d4536fa7d0_molmim.vocab\n",
      "[NeMo I 2024-05-23 13:57:03 regex_tokenizer:254] Loading regex from file = /tmp/tmpchettrbs/048c1f797f464dd5b6a90f60f9405827_molmim.model\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_base_model:315] Padded vocab_size: 640, original vocab_size: 523, dummy tokens: 117.\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_hiddens:121] NOTE: Adding hiddens transforms and losses\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_hiddens:149] Added transform q_z_given_x with cfg={'cls_name': 'sampled_var_cond_gaussian', 'hidden_size': 512, 'min_logvar': -6.0, 'max_logvar': 0.0, 'map_var_to_hiddens': False}\n",
      "[NeMo I 2024-05-23 13:57:03 megatron_hiddens:177] Added loss mim with cfg={'cls_name': 'a_mim', 'loss_weight': 1.0}\n",
      "[NeMo I 2024-05-23 13:57:03 nlp_overrides:752] Model MolMIMModel was successfully restored from /workspace/bionemo/models/molecule/molmim/molmim_70m_24_3.nemo.\n",
      "[NeMo I 2024-05-23 13:57:04 megatron_lm_encoder_decoder_model:1195] Decoding using the greedy-search method...\n",
      "Loaded a <class 'bionemo.model.molecule.molmim.infer.MolMIMInference'>\n"
     ]
    }
   ],
   "source": [
    "from bionemo.triton.utils import load_model_for_inference\n",
    "from bionemo.model.molecule.molmim.infer import MolMIMInference\n",
    "\n",
    "inferer = load_model_for_inference(cfg, interactive=True)\n",
    "\n",
    "print(f\"Loaded a {type(inferer)}\")\n",
    "assert isinstance(inferer, MolMIMInference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f5a9d",
   "metadata": {},
   "source": [
    "### SMILES to Embedding\n",
    "\n",
    "`smis_to_embedding` queries the model to fetch the encoder embedding for the input SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d24ee9b9-e78e-454e-af1f-830f8e3035cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29030e64-0b18-420e-bfed-b87495e9e685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def canonicalize_smiles(smiles: str) -> str:\n",
    "    \"\"\"Canonicalize input SMILES\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    canon_smiles = Chem.MolToSmiles(mol, canonical=True)\n",
    "    return canon_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1249a4c-7b96-4f1b-9a13-7f9334b56235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = 'c1cc2ccccc2cc1'\n",
    "cs = canonicalize_smiles(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d5d36-98bb-460e-9d32-6a836eef15c8",
   "metadata": {},
   "source": [
    "# Test 1: absolute difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98b9d7ac-f7f6-4f92-9016-3c0c969c52a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smis1 = [s] # original\n",
    "smis2 = [cs] # original canoicalized\n",
    "smis3 = [s, s] # orignial\n",
    "smis4 = [s,  'COc1cc2nc(N3CCN(C(=O)c4ccco4)CC3)nc(N)c2cc1OC'] # orignianl, random mol\n",
    "smis5 = [s,  'CC(C)CC1=CC=C(C=C1)C2=CC=CC=C2C3=CC=C(C=C3)C4=CC=CC=C4'] # original, random\n",
    "smis6 = [s, 'Nc1nc(cs1)C(=NOCC(O)=O)C(=O)N[C@H]1[C@H]2SCC(C=C)=C(N2C1=O)C(O)=O', 'COc1cc2nc(N3CCN(C(=O)c4ccco4)CC3)nc(N)c2cc1OC'] # original, rand, rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3799a7e-fdff-457f-95e7-57d43d0b6d24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test1 = inferer.seq_to_embeddings(smis1)[0, :].cpu()\n",
    "test2 = inferer.seq_to_embeddings(smis2)[0, :].cpu()\n",
    "test3 = inferer.seq_to_embeddings(smis3)[0, :].cpu()\n",
    "test4 = inferer.seq_to_embeddings(smis4)[0, :].cpu()\n",
    "test5 = inferer.seq_to_embeddings(smis5)[0, :].cpu()\n",
    "test6 = inferer.seq_to_embeddings(smis6)[0, :].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3219de0a-582d-4837-9894-4095701bc590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original [s] vs canonicalized mols [cs] produce very different embeddings\n",
    "np.allclose(test1, test2, atol=1e-01, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "942933da-becc-4320-a86b-f5c96e208618",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [s] vs [s, s] generates similar embeddings for s, up to a certain threshold 1e-3\n",
    "np.allclose(test1, test3, atol=1e-03, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "066fd48a-bcbf-4f54-96f6-a9dc62dcb770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [s] vs [s, random] generates simlar embeddings for s, up to a certain threshold 1e-3\n",
    "np.allclose(test1, test4, atol=1e-03, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9988c1a1-8657-4c79-9501-eb196642a515",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [s] vs [s, random] generates simlar embeddings for s, up to a certain threshold 1e-3\n",
    "np.allclose(test1, test5, atol=1e-03, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80aca7db-6b00-4f16-a781-c7524f230bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [s] vs [s, random, random] generates simlar embeddings for s, up to a certain threshold 1e-3\n",
    "\n",
    "np.allclose(test1, test5, atol=1e-03, rtol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d195535-368a-46ac-85bb-a362beba38fe",
   "metadata": {},
   "source": [
    "# Test 2: cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7f4a766-e543-4afa-810c-c853b456e428",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5625"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [s] vs [cs]\n",
    "cos_sim = cosine_similarity(test1.reshape(-1, 1), test2.reshape(-1, 1))\n",
    "np.mean(np.diag(cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "392ccc9c-233a-4beb-ac18-50d696d6a956",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(test1.reshape(-1, 1), test3.reshape(-1, 1))\n",
    "np.mean(np.diag(cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64eca9f2-1f4b-4782-ab99-e1c0211e39b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(test1.reshape(-1, 1), test4.reshape(-1, 1))\n",
    "np.mean(np.diag(cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e99b0c7-e175-4ef0-9b43-24497bb99abe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(test1.reshape(-1, 1), test5.reshape(-1, 1))\n",
    "np.mean(np.diag(cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c8b62eb6-9aca-45f6-b4f2-bb2fdd71abca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(test1.reshape(-1, 1), test6.reshape(-1, 1))\n",
    "np.mean(np.diag(cos_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730010e-9a3c-4242-a885-56cb1ef957a6",
   "metadata": {},
   "source": [
    "# Test 3: How to always generate the same result for a given molecule\n",
    "\n",
    "You will need use a N=1 batch for inference (e.g. predict 1 molecule each time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57fffaa8-b101-42e2-b0fa-dc14a85091a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "for n in range(N): \n",
    "    result = inferer.seq_to_embeddings(smis1)[0, :].cpu()\n",
    "    \n",
    "    # exactly the same array\n",
    "    assert np.array_equal(test1, result)\n",
    "    \n",
    "    # cosine = 1\n",
    "    assert np.mean(np.diag(cosine_similarity(test1.reshape(-1, 1), result.reshape(-1, 1)))) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68164ff3-bb39-4e8f-896d-79d458b30f00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
