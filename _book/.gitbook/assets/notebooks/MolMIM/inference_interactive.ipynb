{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from rdkit import Chem\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model configuration at: /workspace/bionemo/examples/molecule/molmim/conf\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "try:\n",
    "    BIONEMO_HOME: Path = Path(os.environ['BIONEMO_HOME']).absolute()\n",
    "except KeyError:\n",
    "    print(\"Must have BIONEMO_HOME set in the environment! See docs for instructions.\")\n",
    "    raise\n",
    "\n",
    "config_path = BIONEMO_HOME / \"examples\" / \"molecule\" / \"molmim\" / \"conf\"\n",
    "print(f\"Using model configuration at: {config_path}\")\n",
    "assert config_path.is_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are taking two example SMILES for two widely used Antimalarial drugs -- Mefloquine and Hydroxychloroquine\n",
    "smis = ['OC(c1cc(C(F)(F)F)nc2c(C(F)(F)F)cccc12)C1CCCCN1',     \n",
    "        'CCN(CCO)CCCC(C)Nc1ccnc2cc(Cl)ccc12'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bionemo.utils.hydra import load_model_config\n",
    "\n",
    "cfg = load_model_config(config_name=\"infer.yaml\", config_path=config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rdkit:Enabling RDKit 2023.09.1 jupyter extensions\n",
      "INFO:datasets:PyTorch version 2.1.0a0+32f93b1 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-21 19:17:11 megatron_hiddens:110] Registered hidden transform sampled_var_cond_gaussian at bionemo.model.core.hiddens_support.SampledVarGaussianHiddenTransform\n",
      "[NeMo I 2024-05-21 19:17:11 megatron_hiddens:110] Registered hidden transform interp_var_cond_gaussian at bionemo.model.core.hiddens_support.InterpVarGaussianHiddenTransform\n",
      "[NeMo I 2024-05-21 19:17:11 utils:490] pytorch DDP is not initialized. Initializing with pytorch-lightening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-21 19:17:11 utils:333] Restoring model from /workspace/bionemo/models/molecule/molmim/molmim_70m_24_3.nemo\n",
      "[NeMo I 2024-05-21 19:17:11 utils:337] Loading model class: bionemo.model.molecule.molmim.molmim_model.MolMIMModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive mode selected, using strategy='auto'\n",
      "[NeMo I 2024-05-21 19:17:12 exp_manager:394] Experiments will be logged at /workspace/bionemo/examples/molecule/molmim/nemo_experiments/MolMIM_Inference/2024-05-21_19-17-11\n",
      "[NeMo I 2024-05-21 19:17:12 exp_manager:835] TensorboardLogger has been set up\n",
      "[NeMo I 2024-05-21 19:17:12 utils:306] \n",
      "    \n",
      "    ************** Trainer configuration ***********\n",
      "[NeMo I 2024-05-21 19:17:12 utils:307] \n",
      "    name: MolMIM_Inference\n",
      "    desc: Minimum configuration for initializing a MolMIM model for inference.\n",
      "    trainer:\n",
      "      precision: 16-mixed\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      logger: false\n",
      "      accumulate_grad_batches: 1\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_checkpoint_callback: false\n",
      "    model:\n",
      "      encoder:\n",
      "        num_layers: 6\n",
      "        hidden_size: 512\n",
      "        ffn_hidden_size: 2048\n",
      "        num_attention_heads: 8\n",
      "        init_method_std: 0.02\n",
      "        hidden_dropout: 0.1\n",
      "        attention_dropout: 0.1\n",
      "        ffn_dropout: 0.0\n",
      "        position_embedding_type: learned_absolute\n",
      "        relative_attention_num_buckets: 32\n",
      "        relative_attention_max_distance: 128\n",
      "        relative_position_bias_self_attention_only: true\n",
      "        kv_channels: null\n",
      "        apply_query_key_layer_scaling: false\n",
      "        layernorm_epsilon: 1.0e-05\n",
      "        persist_layer_norm: true\n",
      "        bias_activation_fusion: true\n",
      "        grad_div_ar_fusion: true\n",
      "        masked_softmax_fusion: true\n",
      "        bias_dropout_add_fusion: true\n",
      "        bias: true\n",
      "        normalization: layernorm\n",
      "        arch: perceiver\n",
      "        activation: gelu\n",
      "        headscale: false\n",
      "        transformer_block_type: pre_ln\n",
      "        hidden_steps: 1\n",
      "        num_self_attention_per_cross_attention: 1\n",
      "        openai_gelu: false\n",
      "        onnx_safe: false\n",
      "        fp32_residual_connection: false\n",
      "        activations_checkpoint_method: null\n",
      "        activations_checkpoint_num_layers: 1\n",
      "        activations_checkpoint_granularity: null\n",
      "        megatron_legacy: false\n",
      "        normalize_attention_scores: true\n",
      "        num_moe_experts: 1\n",
      "        moe_frequency: 1\n",
      "        moe_dropout: 0.0\n",
      "        use_flash_attention: false\n",
      "      decoder:\n",
      "        num_layers: 6\n",
      "        hidden_size: 512\n",
      "        ffn_hidden_size: 2048\n",
      "        num_attention_heads: 8\n",
      "        init_method_std: 0.02\n",
      "        hidden_dropout: 0.1\n",
      "        attention_dropout: 0.1\n",
      "        ffn_dropout: 0.0\n",
      "        position_embedding_type: learned_absolute\n",
      "        relative_attention_num_buckets: 32\n",
      "        relative_attention_max_distance: 128\n",
      "        relative_position_bias_self_attention_only: true\n",
      "        kv_channels: null\n",
      "        apply_query_key_layer_scaling: false\n",
      "        layernorm_epsilon: 1.0e-05\n",
      "        persist_layer_norm: true\n",
      "        bias_activation_fusion: true\n",
      "        grad_div_ar_fusion: true\n",
      "        masked_softmax_fusion: true\n",
      "        bias_dropout_add_fusion: true\n",
      "        bias: true\n",
      "        normalization: layernorm\n",
      "        arch: transformer\n",
      "        activation: gelu\n",
      "        headscale: false\n",
      "        transformer_block_type: pre_ln\n",
      "        hidden_steps: 32\n",
      "        num_self_attention_per_cross_attention: 1\n",
      "        openai_gelu: false\n",
      "        onnx_safe: false\n",
      "        fp32_residual_connection: false\n",
      "        activations_checkpoint_method: null\n",
      "        activations_checkpoint_num_layers: 1\n",
      "        activations_checkpoint_granularity: null\n",
      "        megatron_legacy: false\n",
      "        normalize_attention_scores: true\n",
      "        num_moe_experts: 1\n",
      "        moe_frequency: 1\n",
      "        moe_dropout: 0.0\n",
      "        use_flash_attention: false\n",
      "      name: MolMIM-small\n",
      "      micro_batch_size: ${model.data.batch_size}\n",
      "      global_batch_size: 128\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      resume_from_checkpoint: null\n",
      "      pipeline_model_parallel_split_rank: 0\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      megatron_amp_O2: false\n",
      "      seq_length: 128\n",
      "      max_position_embeddings: 128\n",
      "      gradient_as_bucket_view: true\n",
      "      bias_gelu_fusion: true\n",
      "      share_token_embeddings: true\n",
      "      share_decoder_tokens_head_embeddings: false\n",
      "      hidden_size: 512\n",
      "      training_callbacks: []\n",
      "      hiddens:\n",
      "        enc_output_name: z\n",
      "        enc_inference_output_name: z_mean\n",
      "        token_aggregation_method: mean\n",
      "        hidden_aggregation_method: mean\n",
      "        transform:\n",
      "          q_z_given_x:\n",
      "            cls_name: sampled_var_cond_gaussian\n",
      "            hidden_size: 512\n",
      "            min_logvar: -6.0\n",
      "            max_logvar: 0.0\n",
      "            map_var_to_hiddens: false\n",
      "        loss:\n",
      "          mim:\n",
      "            cls_name: a_mim\n",
      "            loss_weight: 1.0\n",
      "      tokenizer:\n",
      "        library: regex\n",
      "        type: null\n",
      "        model: nemo:048c1f797f464dd5b6a90f60f9405827_molmim.model\n",
      "        vocab_file: nemo:dd344353154640acbbaea1d4536fa7d0_molmim.vocab\n",
      "        merge_file: null\n",
      "        vocab_path: ${oc.env:BIONEMO_HOME}/tokenizers/molecule/molmim/vocab/molmim.vocab\n",
      "        model_path: ${oc.env:BIONEMO_HOME}/tokenizers/molecule/molmim/vocab/molmim.model\n",
      "      data:\n",
      "        links_file: /workspace/bionemo/examples/molecule/megamolbart/dataset/ZINC-downloader.txt\n",
      "        dataset_path: ${oc.env:BIONEMO_HOME}/examples/tests/test_data/molecule/physchem/SAMPL/test/x000\n",
      "        dataset:\n",
      "          train: x_OP_000..175_CL_\n",
      "          test: x_OP_000..175_CL_\n",
      "          val: x_OP_000..004_CL_\n",
      "        canonicalize_target_smile: true\n",
      "        canonicalize_encoder_input: true\n",
      "        canonicalize_decoder_output: true\n",
      "        encoder_augment: false\n",
      "        decoder_independent_augment: false\n",
      "        encoder_mask: false\n",
      "        decoder_mask: false\n",
      "        mask_prob: 0.0\n",
      "        span_lambda: 3.0\n",
      "        micro_batch_size: 2048\n",
      "        num_workers: 4\n",
      "        dataloader_type: single\n",
      "        max_seq_length: 128\n",
      "        seed: 42\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        data_impl: ''\n",
      "        index_mapping_type: online\n",
      "        data_impl_kwargs:\n",
      "          csv_mmap:\n",
      "            newline_int: 10\n",
      "            header_lines: 1\n",
      "            workers: 10\n",
      "            sort_dataset_paths: true\n",
      "            data_sep: ','\n",
      "            data_col: 1\n",
      "          csv_fields_mmap:\n",
      "            newline_int: 10\n",
      "            header_lines: 1\n",
      "            workers: null\n",
      "            sort_dataset_paths: false\n",
      "            data_sep: ','\n",
      "            data_fields:\n",
      "              id: 0\n",
      "              sequence: 1\n",
      "          fasta_fields_mmap:\n",
      "            data_fields:\n",
      "              id: 0\n",
      "              sequence: 1\n",
      "        use_upsampling: true\n",
      "        index_mapping_dir: null\n",
      "        batch_size: 128\n",
      "        output_fname: ''\n",
      "        data_fields_map:\n",
      "          sequence: smiles\n",
      "          id: iupac\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0005\n",
      "        weight_decay: 0.001\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 10000.0\n",
      "          constant_steps: 50000.0\n",
      "          max_steps: 1000000\n",
      "          min_lr: 5.0e-05\n",
      "      dwnstr_task_validation:\n",
      "        enabled: false\n",
      "        dataset:\n",
      "          class: bionemo.model.core.dwnstr_task_callbacks.SingleValuePredictionCallback\n",
      "          task_type: regression\n",
      "          infer_target: bionemo.model.molecule.molmim.infer.MolMIMInference\n",
      "          max_seq_length: 128\n",
      "          emb_batch_size: 128\n",
      "          batch_size: 128\n",
      "          num_epochs: 10\n",
      "          shuffle: true\n",
      "          num_workers: 8\n",
      "          dataset_path: /data/physchem/\n",
      "          task_name: SAMPL\n",
      "          dataset:\n",
      "            train: x000\n",
      "            test: x000\n",
      "          sequence_column: smiles\n",
      "          target_column: expt\n",
      "          random_seed: 1234\n",
      "          optim:\n",
      "            name: adam\n",
      "            lr: 0.0001\n",
      "            betas:\n",
      "            - 0.9\n",
      "            - 0.999\n",
      "            eps: 1.0e-08\n",
      "            weight_decay: 0.01\n",
      "            sched:\n",
      "              name: WarmupAnnealing\n",
      "              min_lr: 1.0e-05\n",
      "              last_epoch: -1\n",
      "              warmup_ratio: 0.01\n",
      "              max_steps: 1000\n",
      "      precision: 32\n",
      "      target: bionemo.model.molecule.molmim.molmim_model.MolMIMModel\n",
      "      nemo_version: 1.22.0\n",
      "      downstream_task:\n",
      "        restore_from_path: ${oc.env:BIONEMO_HOME}/models/molecule/molmim/molmim_70m_24_3.nemo\n",
      "        outputs:\n",
      "        - embeddings\n",
      "    target: bionemo.model.molecule.molmim.molmim_model.MolMIMModel\n",
      "    infer_target: bionemo.model.molecule.molmim.infer.MolMIMInference\n",
      "    formatters:\n",
      "      simple:\n",
      "        format: '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'\n",
      "    handlers:\n",
      "      console:\n",
      "        class: logging.StreamHandler\n",
      "        formatter: simple\n",
      "        stream: ext://sys.stdout\n",
      "      file:\n",
      "        class: logging.FileHandler\n",
      "        formatter: simple\n",
      "        filename: /logs/inference.log\n",
      "    root:\n",
      "      level: INFO\n",
      "      handlers:\n",
      "      - console\n",
      "    disable_existing_loggers: false\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-21 19:17:12 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_init:297] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 megatron_base_model:821] The model: MolMIMModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-21 19:17:12 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-21 19:17:12 tokenizer_utils:199] Using regex tokenization\n",
      "[NeMo I 2024-05-21 19:17:12 regex_tokenizer:240] Loading vocabulary from file = /tmp/tmpxac6vhjf/dd344353154640acbbaea1d4536fa7d0_molmim.vocab\n",
      "[NeMo I 2024-05-21 19:17:12 regex_tokenizer:254] Loading regex from file = /tmp/tmpxac6vhjf/048c1f797f464dd5b6a90f60f9405827_molmim.model\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_base_model:315] Padded vocab_size: 640, original vocab_size: 523, dummy tokens: 117.\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_hiddens:121] NOTE: Adding hiddens transforms and losses\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_hiddens:149] Added transform q_z_given_x with cfg={'cls_name': 'sampled_var_cond_gaussian', 'hidden_size': 512, 'min_logvar': -6.0, 'max_logvar': 0.0, 'map_var_to_hiddens': False}\n",
      "[NeMo I 2024-05-21 19:17:12 megatron_hiddens:177] Added loss mim with cfg={'cls_name': 'a_mim', 'loss_weight': 1.0}\n",
      "[NeMo I 2024-05-21 19:17:12 nlp_overrides:752] Model MolMIMModel was successfully restored from /workspace/bionemo/models/molecule/molmim/molmim_70m_24_3.nemo.\n",
      "[NeMo I 2024-05-21 19:17:14 megatron_lm_encoder_decoder_model:1195] Decoding using the greedy-search method...\n",
      "Loaded a <class 'bionemo.model.molecule.molmim.infer.MolMIMInference'>\n"
     ]
    }
   ],
   "source": [
    "from bionemo.triton.utils import load_model_for_inference\n",
    "from bionemo.model.molecule.molmim.infer import MolMIMInference\n",
    "\n",
    "inferer = load_model_for_inference(cfg, interactive=True)\n",
    "\n",
    "print(f\"Loaded a {type(inferer)}\")\n",
    "assert isinstance(inferer, MolMIMInference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMILES to hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MolMIM hidden state has a controlled number of tokens since it uses a Perceiver encoder, so no pooling is necessary to create a fixed size embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_states.shape=torch.Size([2, 1, 512])\n",
      "pad_masks.shape=torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "hidden_states, pad_masks = inferer.seq_to_hiddens(smis)\n",
    "print(f\"{hidden_states.shape=}\")\n",
    "print(f\"{pad_masks.shape=}\")\n",
    "\n",
    "assert tuple(hidden_states.shape) == (2, 1, 512)\n",
    "assert tuple(pad_masks.shape) == (2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMILES to embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.shape=torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "embedding = inferer.seq_to_embeddings(smis)\n",
    "print(f\"{embedding.shape=}\")\n",
    "assert tuple(embedding.shape) == (2, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2774,  0.0274, -0.0518,  ..., -0.5057,  0.1316,  0.4939]],\n",
       "\n",
       "        [[-0.1865,  0.1537,  0.4342,  ...,  0.4155, -0.5273,  0.3010]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2774,  0.0274, -0.0518,  ..., -0.5057,  0.1316,  0.4939],\n",
       "        [-0.1865,  0.1537,  0.4342,  ...,  0.4155, -0.5273,  0.3010]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden state to SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-21 20:08:16 megatron_lm_encoder_decoder_model:1195] Decoding using the greedy-search method...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['OC(c1cc(C(F)(F)F)nc2c(C(F)(F)F)cccc12)C1CCCC1',\n",
       " 'CCN(CCO)CCCC(C)Nc1ccnc2cc(Cl)ccc12']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining SMILES chemical representation from a hidden state using the hidden_to_seqs function \n",
    "inferred_smis = inferer.hiddens_to_seq(hidden_states, pad_masks)\n",
    "\n",
    "# Examine the inferred SMILES\n",
    "inferred_smis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling: Generate SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-21 21:01:11 megatron_lm_encoder_decoder_model:1192] Decoding using the beam search method with beam size=5...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 samples\n",
      "Number of mols generated for input 1: 19\n",
      "Generated molecules: \n",
      "['OC(c1ccc(C(F)(F)F)nc1)C(F)(F)F', 'OCc1nsc(N2CCC(CO)(c3ccccc3)CC2)n1', 'COc1ncc([C@H](O)C(=O)c2cc(C(F)(F)F)ccc2F)cc1Cl', 'OC(c1cc(F)c(Cl)cc1Br)N(CC(F)(F)F)C1CC1', 'CCOc1cc(C(=O)N2CC[C@@](O)(C(F)(F)F)C2)ccc1C', 'OC(c1ccccc1)c1cn(C(F)(F)F)nc1C(F)(F)F', 'OCc1ccc(C(F)(F)F)nc1N(Cc1c(F)cccc1F)C1CCCCC1', 'COc1ncc(CN2CC([C@H](C)NC(=O)[C@]3(C)CCC[C@H]3C)C2)c(C)n1', 'OC(c1cc(C(F)(F)F)nc2c(C(F)(F)F)cccc12)C1CCCC1', 'OC[C@@]1(CN(CC(F)(F)F)C2CCC2)CCCO1', 'OC(c1cc(C(F)(F)F)nc(-c2ccc(Cl)cc2)n1)C1CCNCC1', 'Oc1nc(C(O)c2nc3ccccc3s2)cc(C(F)(F)F)n1', 'O=C(c1cccnn1)N1CCCC12CN(C(O)c1cc(C(F)(F)F)ncc1Cl)C2', 'OCc1ccc(C(F)(F)F)nc1N(CC(F)(F)F)C1CCCC1', 'OCCc1cc(C(F)(F)F)nc2c(C(F)(F)F)cccc12', 'OC(c1cc(C(F)(F)F)n[nH]1)(C1CC1)C(F)(F)F', 'OC[C@@]1(O)CC[C@H]2CN(Cc3nc4c(C(F)(F)F)cccc4o3)C[C@@H]2C1', 'OCc1ccc(C(F)(F)F)nc1N(Cc1ccccc1)C[C@@H]1CCCO1', 'OC(c1cc(C(F)(F)F)nc2c(C(F)(F)F)cccc12)C1CCNCC1']\n",
      "Number of mols generated for input 2: 11\n",
      "Generated molecules: \n",
      "['CCN(CCO)CCCCCCNCc1ccnc2cc(Cl)ccc12', 'CCN(CCO)CCC[C@H](C)Nc1ccnc2cc(Cl)ccc12', 'CCN(CCO)CCNC(=O)c1ccnc2cc(Cl)ccc12', 'CCN(CCO)CCCC(=O)NNc1nc2cc(Cl)ccc2s1', 'CCN(CCO)CCCC[C@@H](C)Nc1cnc2cc(Cl)ccc2n1', 'CCN(CCO)CCC[C@@H](C)NC(=O)c1cc(Cl)cc2cccnc12', 'C[C@@H]1C[C@@H](O)CN(C(=O)c2ccnc3cc(Cl)ccc23)C1', 'CCN(CCO)C(=O)[C@@H](Nc1ccnc2cc(Cl)ccc12)C(C)C', 'CCN(CCO)CCCC(=O)Nc1cnc2cc(Cl)ccc2c1', 'CCN(CCO)CCCC(=O)N[C@H](C)C1CN(CCOC)C1', 'CCN(CCO)CCCCN(C)c1ccnc2cc(Cl)ccc12']\n"
     ]
    }
   ],
   "source": [
    "samples = inferer.sample(\n",
    "    num_samples = 20,       # Maximum number of generated molecules per query compound\n",
    "    scaled_radius = 0.7,    # Radius of exploration [range: 0.0 - 1.0] --- the extent of perturbation of the original hidden state for sampling\n",
    "    sampling_method=\"beam-search-perturbate\", \n",
    "    sampler_kwargs = {\n",
    "        \"beam_size\": 3, \"keep_only_best_tokens\": True, \"return_scores\": False\n",
    "    },\n",
    "    seqs=smis\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(samples)} samples\")\n",
    "\n",
    "uniq_canonical_smiles = []\n",
    "for smis_samples, original in zip(samples, smis):\n",
    "    smis_samples = set(smis_samples) - set([original])  # unique strings that are not the same as we started from\n",
    "    valid_molecules = []\n",
    "    for sample in smis_samples:\n",
    "        mol = Chem.MolFromSmiles(sample)\n",
    "        if mol:\n",
    "            valid_molecules.append(Chem.MolToSmiles(mol,True))\n",
    "    uniq_canonical_smiles.append(valid_molecules)\n",
    "\n",
    "for i,s in enumerate(uniq_canonical_smiles):\n",
    "    print(f'Number of mols generated for input {i+1}: {len(s)}')\n",
    "    print('Generated molecules: ')\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
