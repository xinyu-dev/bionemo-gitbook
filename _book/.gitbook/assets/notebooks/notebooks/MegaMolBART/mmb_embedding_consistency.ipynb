{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b43cec",
   "metadata": {},
   "source": [
    "# MegaMolBART Embedding Consistency\n",
    "\n",
    "Make sure you have weights in `/workspace/bionemo/models`. This notebooks uses the interactive inference method without the need to launch local triton server explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1ca44898eb4ca64",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f92202a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model configuration at: /workspace/bionemo/examples/molecule/megamolbart/conf\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "try:\n",
    "    BIONEMO_HOME: Path = Path(os.environ['BIONEMO_HOME']).absolute()\n",
    "except KeyError:\n",
    "    print(\"Must have BIONEMO_HOME set in the environment! See docs for instructions.\")\n",
    "    raise\n",
    "\n",
    "config_path = BIONEMO_HOME / \"examples\" / \"molecule\" / \"megamolbart\" / \"conf\"\n",
    "print(f\"Using model configuration at: {config_path}\")\n",
    "assert config_path.is_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38725cbe",
   "metadata": {},
   "source": [
    "### Setup and Test Data\n",
    "\n",
    "`InferenceWrapper` is an adaptor that allows interaction with inference service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbf1a985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bionemo.utils.hydra import load_model_config\n",
    "\n",
    "cfg = load_model_config(config_name=\"infer.yaml\", config_path=config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16f30d8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-22 21:52:55 utils:490] pytorch DDP is not initialized. Initializing with pytorch-lightening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-22 21:52:55 utils:333] Restoring model from /workspace/bionemo/models/molecule/megamolbart/megamolbart.nemo\n",
      "[NeMo I 2024-05-22 21:52:55 utils:337] Loading model class: bionemo.model.molecule.megamolbart.megamolbart_model.MegaMolBARTModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive mode selected, using strategy='auto'\n",
      "[NeMo I 2024-05-22 21:52:56 exp_manager:394] Experiments will be logged at /workspace/bionemo/examples/molecule/megamolbart/nbs/nemo_experiments/MegaMolBART_Inference/2024-05-22_21-47-12\n",
      "[NeMo I 2024-05-22 21:52:56 exp_manager:835] TensorboardLogger has been set up\n",
      "[NeMo I 2024-05-22 21:52:56 utils:306] \n",
      "    \n",
      "    ************** Trainer configuration ***********\n",
      "[NeMo I 2024-05-22 21:52:56 utils:307] \n",
      "    name: MegaMolBART_Inference\n",
      "    desc: Minimum configuration for initializing a MegaMolBART model for inference.\n",
      "    trainer:\n",
      "      precision: 16-mixed\n",
      "      devices: 1\n",
      "      num_nodes: 1\n",
      "      accelerator: gpu\n",
      "      logger: false\n",
      "      accumulate_grad_batches: 1\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_checkpoint_callback: false\n",
      "    model:\n",
      "      name: small_span_aug\n",
      "      micro_batch_size: ${model.data.batch_size}\n",
      "      global_batch_size: 128\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      resume_from_checkpoint: null\n",
      "      pipeline_model_parallel_split_rank: 0\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      megatron_amp_O2: false\n",
      "      seq_length: 512\n",
      "      max_position_embeddings: 512\n",
      "      num_layers: 6\n",
      "      hidden_size: 512\n",
      "      ffn_hidden_size: 2048\n",
      "      num_attention_heads: 8\n",
      "      init_method_std: 0.02\n",
      "      hidden_dropout: 0.1\n",
      "      attention_dropout: 0.1\n",
      "      position_embedding_type: learned_absolute\n",
      "      relative_position_bias_self_attention_only: true\n",
      "      relative_attention_num_buckets: 32\n",
      "      relative_attention_max_distance: 128\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      persist_layer_norm: true\n",
      "      gradient_as_bucket_view: true\n",
      "      bias_gelu_fusion: true\n",
      "      masked_softmax_fusion: true\n",
      "      bias_dropout_add_fusion: true\n",
      "      bias: true\n",
      "      normalization: layernorm\n",
      "      encoder_arch: transformer\n",
      "      decoder_arch: transformer\n",
      "      activation: gelu\n",
      "      headscale: false\n",
      "      share_token_embeddings: true\n",
      "      share_decoder_tokens_head_embeddings: false\n",
      "      tokenizer:\n",
      "        library: regex\n",
      "        type: null\n",
      "        model: nemo:111b90cc2819425382967ab999101096_megamolbart.model\n",
      "        vocab_file: nemo:36b36f49c3e64962a7b54f1a1ba2b580_megamolbart.vocab\n",
      "        merge_file: null\n",
      "        vocab_path: ${oc.env:BIONEMO_HOME}/tokenizers/molecule/megamolbart/vocab/megamolbart.vocab\n",
      "        model_path: ${oc.env:BIONEMO_HOME}/tokenizers/molecule/megamolbart/vocab/megamolbart.model\n",
      "      data:\n",
      "        links_file: /workspace/bionemo/examples/molecule/megamolbart/dataset/ZINC-downloader.txt\n",
      "        dataset_path: ${oc.env:BIONEMO_HOME}/examples/tests/test_data/molecule/physchem/SAMPL/test/x000\n",
      "        dataset:\n",
      "          train: x000\n",
      "          test: x000\n",
      "          val: x000\n",
      "        encoder_augment: false\n",
      "        encoder_mask: true\n",
      "        decoder_augment: true\n",
      "        decoder_mask: false\n",
      "        mask_prob: 0.1\n",
      "        span_lambda: 3.0\n",
      "        micro_batch_size: 32\n",
      "        num_workers: 4\n",
      "        dataloader_type: single\n",
      "        canonicalize_input: true\n",
      "        max_seq_length: 512\n",
      "        seed: 42\n",
      "        skip_lines: 0\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        data_impl: ''\n",
      "        data_impl_kwargs:\n",
      "          csv_mmap:\n",
      "            newline_int: 10\n",
      "            header_lines: 1\n",
      "            workers: 10\n",
      "            sort_dataset_paths: true\n",
      "            data_sep: ','\n",
      "            data_col: 1\n",
      "          csv_fields_mmap:\n",
      "            newline_int: 10\n",
      "            header_lines: 1\n",
      "            workers: null\n",
      "            sort_dataset_paths: false\n",
      "            data_sep: ','\n",
      "            data_fields:\n",
      "              id: 0\n",
      "              sequence: 1\n",
      "          fasta_fields_mmap:\n",
      "            data_fields:\n",
      "              id: 0\n",
      "              sequence: 1\n",
      "        use_upsampling: true\n",
      "        batch_size: 128\n",
      "        output_fname: ''\n",
      "        index_mapping_dir: null\n",
      "        data_fields_map:\n",
      "          sequence: smiles\n",
      "          id: iupac\n",
      "        canonicalize_target_smile: true\n",
      "        canonicalize_encoder_input: false\n",
      "        canonicalize_decoder_output: false\n",
      "        decoder_independent_augment: false\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 1.0\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.999\n",
      "        eps: 1.0e-08\n",
      "        weight_decay: 0.01\n",
      "        sched:\n",
      "          name: NoamAnnealing\n",
      "          d_model: 512\n",
      "          warmup_steps: 8000\n",
      "          warmup_ratio: null\n",
      "          max_steps: 1000000\n",
      "          min_lr: 1.0e-05\n",
      "      precision: 16\n",
      "      target: bionemo.model.molecule.megamolbart.megamolbart_model.MegaMolBARTModel\n",
      "      nemo_version: 1.18.1\n",
      "      encoder:\n",
      "        name: small_span_aug\n",
      "        micro_batch_size: 32\n",
      "        global_batch_size: 32\n",
      "        tensor_model_parallel_size: 1\n",
      "        pipeline_model_parallel_size: 1\n",
      "        resume_from_checkpoint: null\n",
      "        pipeline_model_parallel_split_rank: 0\n",
      "        make_vocab_size_divisible_by: 128\n",
      "        pre_process: true\n",
      "        post_process: true\n",
      "        megatron_amp_O2: false\n",
      "        seq_length: 512\n",
      "        max_position_embeddings: 512\n",
      "        num_layers: 6\n",
      "        hidden_size: 512\n",
      "        ffn_hidden_size: 2048\n",
      "        num_attention_heads: 8\n",
      "        init_method_std: 0.02\n",
      "        hidden_dropout: 0.1\n",
      "        attention_dropout: 0.1\n",
      "        position_embedding_type: learned_absolute\n",
      "        relative_position_bias_self_attention_only: true\n",
      "        relative_attention_num_buckets: 32\n",
      "        relative_attention_max_distance: 128\n",
      "        kv_channels: null\n",
      "        apply_query_key_layer_scaling: true\n",
      "        layernorm_epsilon: 1.0e-05\n",
      "        persist_layer_norm: true\n",
      "        gradient_as_bucket_view: true\n",
      "        bias_gelu_fusion: true\n",
      "        masked_softmax_fusion: true\n",
      "        bias_dropout_add_fusion: true\n",
      "        bias: true\n",
      "        normalization: layernorm\n",
      "        encoder_arch: transformer\n",
      "        decoder_arch: transformer\n",
      "        activation: gelu\n",
      "        headscale: false\n",
      "        share_token_embeddings: true\n",
      "        share_decoder_tokens_head_embeddings: false\n",
      "        tokenizer:\n",
      "          library: regex\n",
      "          type: null\n",
      "          model: /tokenizers/molecule/megamolbart/vocab/megamolbart.model\n",
      "          vocab_file: /tokenizers/molecule/megamolbart/vocab/megamolbart.vocab\n",
      "          merge_file: null\n",
      "        data:\n",
      "          links_file: /workspace/bionemo/examples/molecule/megamolbart/dataset/ZINC-downloader.txt\n",
      "          dataset_path: /data/zinc_csv_test\n",
      "          dataset:\n",
      "            train: x000\n",
      "            test: x000\n",
      "            val: x000\n",
      "          encoder_augment: true\n",
      "          encoder_mask: true\n",
      "          decoder_augment: true\n",
      "          decoder_mask: false\n",
      "          mask_prob: 0.1\n",
      "          span_lambda: 3.0\n",
      "          micro_batch_size: 32\n",
      "          num_workers: 10\n",
      "          dataloader_type: single\n",
      "          canonicalize_input: true\n",
      "          max_seq_length: 512\n",
      "          seed: 42\n",
      "          skip_lines: 0\n",
      "          drop_last: false\n",
      "          pin_memory: false\n",
      "          data_impl: csv_mmap\n",
      "          data_impl_kwargs:\n",
      "            csv_mmap:\n",
      "              newline_int: 10\n",
      "              header_lines: 1\n",
      "              workers: 10\n",
      "              sort_dataset_paths: true\n",
      "              data_sep: ','\n",
      "              data_col: 1\n",
      "          use_upsampling: true\n",
      "        optim:\n",
      "          name: fused_adam\n",
      "          lr: 1.0\n",
      "          betas:\n",
      "          - 0.9\n",
      "          - 0.999\n",
      "          eps: 1.0e-08\n",
      "          weight_decay: 0.01\n",
      "          sched:\n",
      "            name: NoamAnnealing\n",
      "            d_model: 512\n",
      "            warmup_steps: 8000\n",
      "            warmup_ratio: null\n",
      "            max_steps: 1000000\n",
      "            min_lr: 1.0e-05\n",
      "        precision: 16\n",
      "        target: bionemo.model.molecule.megamolbart.megamolbart_model.MegaMolBARTModel\n",
      "        nemo_version: 1.18.1\n",
      "        arch: transformer\n",
      "        bias_activation_fusion: true\n",
      "      decoder:\n",
      "        name: small_span_aug\n",
      "        micro_batch_size: 32\n",
      "        global_batch_size: 32\n",
      "        tensor_model_parallel_size: 1\n",
      "        pipeline_model_parallel_size: 1\n",
      "        resume_from_checkpoint: null\n",
      "        pipeline_model_parallel_split_rank: 0\n",
      "        make_vocab_size_divisible_by: 128\n",
      "        pre_process: true\n",
      "        post_process: true\n",
      "        megatron_amp_O2: false\n",
      "        seq_length: 512\n",
      "        max_position_embeddings: 512\n",
      "        num_layers: 6\n",
      "        hidden_size: 512\n",
      "        ffn_hidden_size: 2048\n",
      "        num_attention_heads: 8\n",
      "        init_method_std: 0.02\n",
      "        hidden_dropout: 0.1\n",
      "        attention_dropout: 0.1\n",
      "        position_embedding_type: learned_absolute\n",
      "        relative_position_bias_self_attention_only: true\n",
      "        relative_attention_num_buckets: 32\n",
      "        relative_attention_max_distance: 128\n",
      "        kv_channels: null\n",
      "        apply_query_key_layer_scaling: true\n",
      "        layernorm_epsilon: 1.0e-05\n",
      "        persist_layer_norm: true\n",
      "        gradient_as_bucket_view: true\n",
      "        bias_gelu_fusion: true\n",
      "        masked_softmax_fusion: true\n",
      "        bias_dropout_add_fusion: true\n",
      "        bias: true\n",
      "        normalization: layernorm\n",
      "        encoder_arch: transformer\n",
      "        decoder_arch: transformer\n",
      "        activation: gelu\n",
      "        headscale: false\n",
      "        share_token_embeddings: true\n",
      "        share_decoder_tokens_head_embeddings: false\n",
      "        tokenizer:\n",
      "          library: regex\n",
      "          type: null\n",
      "          model: /tokenizers/molecule/megamolbart/vocab/megamolbart.model\n",
      "          vocab_file: /tokenizers/molecule/megamolbart/vocab/megamolbart.vocab\n",
      "          merge_file: null\n",
      "        data:\n",
      "          links_file: /workspace/bionemo/examples/molecule/megamolbart/dataset/ZINC-downloader.txt\n",
      "          dataset_path: /data/zinc_csv_test\n",
      "          dataset:\n",
      "            train: x000\n",
      "            test: x000\n",
      "            val: x000\n",
      "          encoder_augment: true\n",
      "          encoder_mask: true\n",
      "          decoder_augment: true\n",
      "          decoder_mask: false\n",
      "          mask_prob: 0.1\n",
      "          span_lambda: 3.0\n",
      "          micro_batch_size: 32\n",
      "          num_workers: 10\n",
      "          dataloader_type: single\n",
      "          canonicalize_input: true\n",
      "          max_seq_length: 512\n",
      "          seed: 42\n",
      "          skip_lines: 0\n",
      "          drop_last: false\n",
      "          pin_memory: false\n",
      "          data_impl: csv_mmap\n",
      "          data_impl_kwargs:\n",
      "            csv_mmap:\n",
      "              newline_int: 10\n",
      "              header_lines: 1\n",
      "              workers: 10\n",
      "              sort_dataset_paths: true\n",
      "              data_sep: ','\n",
      "              data_col: 1\n",
      "          use_upsampling: true\n",
      "        optim:\n",
      "          name: fused_adam\n",
      "          lr: 1.0\n",
      "          betas:\n",
      "          - 0.9\n",
      "          - 0.999\n",
      "          eps: 1.0e-08\n",
      "          weight_decay: 0.01\n",
      "          sched:\n",
      "            name: NoamAnnealing\n",
      "            d_model: 512\n",
      "            warmup_steps: 8000\n",
      "            warmup_ratio: null\n",
      "            max_steps: 1000000\n",
      "            min_lr: 1.0e-05\n",
      "        precision: 16\n",
      "        target: bionemo.model.molecule.megamolbart.megamolbart_model.MegaMolBARTModel\n",
      "        nemo_version: 1.18.1\n",
      "        arch: transformer\n",
      "        bias_activation_fusion: true\n",
      "      tokens_head_bias: false\n",
      "      downstream_task:\n",
      "        restore_from_path: ${oc.env:BIONEMO_HOME}/models/molecule/megamolbart/megamolbart.nemo\n",
      "        outputs:\n",
      "        - embeddings\n",
      "      callbacks: []\n",
      "    target: bionemo.model.molecule.megamolbart.megamolbart_model.MegaMolBARTModel\n",
      "    infer_target: bionemo.model.molecule.megamolbart.infer.MegaMolBARTInference\n",
      "    formatters:\n",
      "      simple:\n",
      "        format: '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'\n",
      "    handlers:\n",
      "      console:\n",
      "        class: logging.StreamHandler\n",
      "        formatter: simple\n",
      "        stream: ext://sys.stdout\n",
      "      file:\n",
      "        class: logging.FileHandler\n",
      "        formatter: simple\n",
      "        filename: /logs/inference.log\n",
      "    root:\n",
      "      level: INFO\n",
      "      handlers:\n",
      "      - console\n",
      "    disable_existing_loggers: false\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-22 21:52:56 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2024-05-22 21:52:56 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-22 21:52:56 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-05-22 21:52:56 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-05-22 21:52:56 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-22 21:52:56 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-05-22 21:52:56 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-22 21:52:56 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-05-22 21:52:56 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-05-22 21:52:56 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-05-22 21:52:56 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-05-22 21:52:56 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-05-22 21:52:56 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-05-22 21:52:56 megatron_init:297] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: sequence_parallel in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: use_cpu_initialization in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 megatron_base_model:821] The model: MegaMolBARTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-05-22 21:52:56 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-22 21:52:56 tokenizer_utils:199] Using regex tokenization\n",
      "[NeMo I 2024-05-22 21:52:56 regex_tokenizer:240] Loading vocabulary from file = /tmp/tmpcfus9d0j/36b36f49c3e64962a7b54f1a1ba2b580_megamolbart.vocab\n",
      "[NeMo I 2024-05-22 21:52:56 regex_tokenizer:254] Loading regex from file = /tmp/tmpcfus9d0j/111b90cc2819425382967ab999101096_megamolbart.model\n",
      "[NeMo I 2024-05-22 21:52:56 megatron_base_model:315] Padded vocab_size: 640, original vocab_size: 523, dummy tokens: 117.\n",
      "[NeMo I 2024-05-22 21:52:56 nlp_overrides:752] Model MegaMolBARTModel was successfully restored from /workspace/bionemo/models/molecule/megamolbart/megamolbart.nemo.\n",
      "[NeMo I 2024-05-22 21:52:56 megatron_lm_encoder_decoder_model:1195] Decoding using the greedy-search method...\n",
      "Loaded a <class 'bionemo.model.molecule.megamolbart.infer.MegaMolBARTInference'>\n"
     ]
    }
   ],
   "source": [
    "from bionemo.triton.utils import load_model_for_inference\n",
    "from bionemo.model.molecule.megamolbart.infer import MegaMolBARTInference\n",
    "\n",
    "inferer = load_model_for_inference(cfg, interactive=True)\n",
    "\n",
    "print(f\"Loaded a {type(inferer)}\")\n",
    "assert isinstance(inferer, MegaMolBARTInference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f5a9d",
   "metadata": {},
   "source": [
    "### SMILES to Embedding\n",
    "\n",
    "`smis_to_embedding` queries the model to fetch the encoder embedding for the input SMILES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d24ee9b9-e78e-454e-af1f-830f8e3035cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "29030e64-0b18-420e-bfed-b87495e9e685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def canonicalize_smiles(smiles: str) -> str:\n",
    "    \"\"\"Canonicalize input SMILES\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    canon_smiles = Chem.MolToSmiles(mol, canonical=True)\n",
    "    return canon_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c1249a4c-7b96-4f1b-9a13-7f9334b56235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = 'c1cc2ccccc2cc1'\n",
    "cs = canonicalize_smiles(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d5d36-98bb-460e-9d32-6a836eef15c8",
   "metadata": {},
   "source": [
    "# Test 1: absolute difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "98b9d7ac-f7f6-4f92-9016-3c0c969c52a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smis1 = [s] # original\n",
    "smis2 = [cs] # original canoicalized\n",
    "smis3 = [s, s] # orignial\n",
    "smis4 = [s,  'COc1cc2nc(N3CCN(C(=O)c4ccco4)CC3)nc(N)c2cc1OC'] # orignianl, random mol\n",
    "smis5 = [s,  'CC(C)CC1=CC=C(C=C1)C2=CC=CC=C2C3=CC=C(C=C3)C4=CC=CC=C4'] # original, random\n",
    "smis6 = [s, 'Nc1nc(cs1)C(=NOCC(O)=O)C(=O)N[C@H]1[C@H]2SCC(C=C)=C(N2C1=O)C(O)=O', 'COc1cc2nc(N3CCN(C(=O)c4ccco4)CC3)nc(N)c2cc1OC'] # original, rand, rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d3799a7e-fdff-457f-95e7-57d43d0b6d24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "test1 = inferer.seq_to_embeddings(smis1)[0, :].cpu()\n",
    "test2 = inferer.seq_to_embeddings(smis2)[0, :].cpu()\n",
    "test3 = inferer.seq_to_embeddings(smis3)[0, :].cpu()\n",
    "test4 = inferer.seq_to_embeddings(smis4)[0, :].cpu()\n",
    "test5 = inferer.seq_to_embeddings(smis5)[0, :].cpu()\n",
    "test6 = inferer.seq_to_embeddings(smis6)[0, :].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3219de0a-582d-4837-9894-4095701bc590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original [s] vs canonicalized mols [cs] produce very different embeddings\n",
    "np.allclose(test1, test2, atol=1e-01, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "942933da-becc-4320-a86b-f5c96e208618",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [s] vs [s, s] generates similar embeddings for s, up to a certain threshold 1e-3\n",
    "np.allclose(test1, test3, atol=1e-03, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "066fd48a-bcbf-4f54-96f6-a9dc62dcb770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [s] vs [s, random] generates simlar embeddings for s, up to a certain threshold 1e-3\n",
    "np.allclose(test1, test4, atol=1e-03, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9988c1a1-8657-4c79-9501-eb196642a515",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [s] vs [s, random] generates simlar embeddings for s, up to a certain threshold 1e-3\n",
    "np.allclose(test1, test5, atol=1e-03, rtol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "80aca7db-6b00-4f16-a781-c7524f230bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [s] vs [s, random, random] generates simlar embeddings for s, up to a certain threshold 1e-3\n",
    "\n",
    "np.allclose(test1, test5, atol=1e-03, rtol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d195535-368a-46ac-85bb-a362beba38fe",
   "metadata": {},
   "source": [
    "# Test 2: cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b7f4a766-e543-4afa-810c-c853b456e428",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9140625"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [s] vs [cs]\n",
    "cos_sim = cosine_similarity(test1.reshape(-1, 1), test2.reshape(-1, 1))\n",
    "np.mean(np.diag(cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "392ccc9c-233a-4beb-ac18-50d696d6a956",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99609375"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(test1.reshape(-1, 1), test3.reshape(-1, 1))\n",
    "np.mean(np.diag(cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "64eca9f2-1f4b-4782-ab99-e1c0211e39b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(test1.reshape(-1, 1), test4.reshape(-1, 1))\n",
    "np.mean(np.diag(cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1e99b0c7-e175-4ef0-9b43-24497bb99abe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(test1.reshape(-1, 1), test5.reshape(-1, 1))\n",
    "np.mean(np.diag(cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c8b62eb6-9aca-45f6-b4f2-bb2fdd71abca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(test1.reshape(-1, 1), test6.reshape(-1, 1))\n",
    "np.mean(np.diag(cos_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730010e-9a3c-4242-a885-56cb1ef957a6",
   "metadata": {},
   "source": [
    "# Test 3: How to always generate the same result for a given molecule\n",
    "\n",
    "You will need use a N=1 batch for inference (e.g. predict 1 molecule each time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "57fffaa8-b101-42e2-b0fa-dc14a85091a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "for n in range(N): \n",
    "    result = inferer.seq_to_embeddings(smis1)[0, :].cpu()\n",
    "    \n",
    "    # exactly the same array\n",
    "    assert np.array_equal(test1, result)\n",
    "    \n",
    "    # cosine = 1\n",
    "    assert np.mean(np.diag(cosine_similarity(test1.reshape(-1, 1), result.reshape(-1, 1)))) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68164ff3-bb39-4e8f-896d-79d458b30f00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
